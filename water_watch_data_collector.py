# -*- coding: utf-8 -*-
"""water_watch_data_collector.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i1fWGlVEs_1dGTzY2Mm6ASBHWomhoUMX
"""

import matplotlib.pyplot as plt
import pandas as pd

import requests
import json

def save_to_csv(streamflow_data, filename='streamflow_data.csv'):
    records = []
    # print(streamflow_data['value']['timeSeries'])
    for timeseries in streamflow_data['value']['timeSeries']:
        # print(json.dumps(timeseries, indent=4))

        site_info = timeseries['sourceInfo']
        values = timeseries['values'][0]

        value = values['value'][0]
        date_time = value['dateTime']
        streamflow_cfs = value['value']

        records.append({
            'site_name': site_info['siteName'],
            'date_time': date_time,
            'streamflow_cfs': streamflow_cfs
        })

    df = pd.DataFrame(records)
    df.to_csv(filename, index=False)

def fetch_streamflow_data(state_code='TN'):
    """
    Fetch streamflow data from the USGS NWIS service.

    Parameters:
    - state_code: State abbreviation (default is 'CA' for California)

    Returns:
    - List containing streamflow data.
    """
    # Base URL for the NWIS API
    base_url = "https://waterservices.usgs.gov/nwis/iv/"

    # Parameters for the API request
    params = {
        'format': 'json',  # Response format
        'stateCd': state_code,  # State Code
        'siteStatus': 'active',  # Only fetch active sites
        'parameterCd': '00060',  # Parameter code for streamflow
    }

    # Send the request to the USGS NWIS service
    response = requests.get(base_url, params=params)

    # Check if the request was successful
    if response.status_code == 200:
        data = response.json()
        return data
    else:
        raise Exception(f"Error fetching data: {response.status_code} - {response.text}")

def fetch_historical_streamflow_data(site_number, start_date, end_date):
    """
    Fetch historical streamflow data from the USGS NWIS service.

    Parameters:
    - site_number: USGS site number for the streamgage.
    - start_date: Start date in the format 'YYYY-MM-DD'.
    - end_date: End date in the format 'YYYY-MM-DD'.

    Returns:
    - DataFrame containing historical streamflow data.
    """
    base_url = "https://waterservices.usgs.gov/nwis/dv/"

    # Parameters for the API request
    params = {
        'format': 'json',
        'sites': site_number,
        'startDT': start_date,
        'endDT': end_date,
        'parameterCd': '00060',
        'siteStatus': 'active'
    }

    response = requests.get(base_url, params=params)

    if response.status_code == 200:
        data = response.json()
        # Parse the JSON data into a DataFrame
        time_series = data['value']['timeSeries']
        records = []

        # How the json is formatted inside each series
        """
        "values": [
            {
                "value": [
                    {
                        "value": "335",
                        "qualifiers": [
                            "A"
                        ],
                        "dateTime": "2020-01-01T00:00:00.000"
                    },
        """
        for series in time_series:
            values = series['values'][0]["value"]
            for occurrence in values:
                record = {
                    'date_time': occurrence["dateTime"],
                    'streamflow_cfs': occurrence["value"]
                }
                records.append(record)

        df = pd.DataFrame(records)
        if len(records) != 0:
            df['date_time'] = pd.to_datetime(df['date_time'])
        else:
            print(df.head())
        return df
    else:
        raise Exception(f"Error fetching historical data: {response.status_code} - {response.text}")

def main():
    try:
        # Fetch streamflow data
        streamflow_data = fetch_streamflow_data()
        # print(json.dumps(streamflow_data, indent=4))
        save_to_csv(streamflow_data)

        site_number = '08166250'        # Brazos River at Richmond, TX (or optionally a gauge on Guadalupe River)
        start_date = '2020-01-01'
        end_date = '2025-07-02'

        historical_streamflow_data = fetch_historical_streamflow_data(site_number, start_date, end_date)

        # Save historical data to CSV
        historical_streamflow_data.to_csv('historical_streamflow_data.csv', index=False)
    except Exception as e:
        print("Error")
        print(e)

main()

def plot_streamflow(df):
    df['date_time'] = pd.to_datetime(df['date_time'], utc=True)

    # Filter the DataFrame to include only rows for today and drop negative streamflow values
    today = pd.Timestamp.today().normalize()

    # Keep only entries that match today's date and are non-negative
    df_filtered = df[
        (df['date_time'].dt.year == today.year) &
        (df['date_time'].dt.month == today.month) &
        (df['date_time'].dt.day == today.day) &
        (df['streamflow_cfs'] >= 0)
    ]

    # Print filtered date_time and streamflow for debugging
    print(df_filtered[['date_time', 'streamflow_cfs']])

    # Plotting
    plt.figure(figsize=(12, 6))
    plt.scatter(df_filtered["date_time"], df_filtered['streamflow_cfs'], label='Streamflow (cfs)', color='b')
    plt.title('Streamflow Over Time')
    plt.xlabel('Date')
    plt.ylabel('Streamflow (cfs)')
    plt.legend()
    plt.grid()
    plt.show()

# Call this function after loading data into DataFrame
plot_streamflow(pd.read_csv('streamflow_data.csv'))

def plot_historical_streamflow(df):
    """
    Plot historical streamflow data.

    Parameters:
    - df: DataFrame containing historical streamflow data with 'date_time' and 'streamflow_cfs' columns.
    """
    # Ensure the 'date_time' column is in datetime format
    df['date_time'] = pd.to_datetime(df['date_time'])

    # Set 'date_time' as index for easy plotting
    df.set_index('date_time', inplace=True)

    plt.figure(figsize=(14, 7))
    plt.plot(df.index, df['streamflow_cfs'], label='Streamflow (cfs)', color='blue', linewidth=2)
    plt.title('Historical Streamflow Over Time')
    plt.xlabel('Date')
    plt.ylabel('Streamflow (cfs)')
    plt.legend()
    plt.grid()
    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
    plt.tight_layout()  # Adjust layout to prevent clipping of labels
    plt.show()

plot_historical_streamflow(pd.read_csv('historical_streamflow_data.csv'))

"""# Task
Rewrite the flash flood prediction model architecture to incorporate percentile-based thresholds and temporal discharge gradients as features, retrain the model, and implement a simple prediction function.

## Feature engineering with percentile thresholds and temporal gradients

### Subtask:
Modify the existing feature engineering code to calculate percentile-based thresholds and temporal discharge gradients (rate-of-change metrics) and add them as new features to the dataset.

**Reasoning**:
Load the historical streamflow data, convert the date_time column to datetime objects, calculate rolling percentiles and temporal gradients, add them as new features, handle missing values, and save the updated DataFrame to a new CSV file.
"""

main()

# Load the historical streamflow data
df = pd.read_csv('historical_streamflow_data.csv')

# Ensure the 'date_time' column is converted to datetime objects
df['date_time'] = pd.to_datetime(df['date_time'])

# Set 'date_time' as the index
df.set_index('date_time', inplace=True)

# Sort the dataframe by index (date_time) to ensure correct rolling calculations
df.sort_index(inplace=True)

# Determine an appropriate window size (e.g., 24 hours for hourly data, or a few days for daily data)
# Assuming daily data for now, let's use a 7-day rolling window
window_size = '7D'

# Calculate rolling percentiles
df['streamflow_p10'] = df['streamflow_cfs'].rolling(window=window_size).quantile(0.10)
df['streamflow_p50'] = df['streamflow_cfs'].rolling(window=window_size).quantile(0.50)
df['streamflow_p90'] = df['streamflow_cfs'].rolling(window=window_size).quantile(0.90)

# Calculate temporal gradient (raw difference and percentage change)
df['streamflow_diff'] = df['streamflow_cfs'].diff()
df['streamflow_pct_change'] = df['streamflow_cfs'].pct_change()

# Handle missing values created by rolling window and diff/pct_change
# For simplicity, we can drop the initial rows with NaN values
df.dropna(inplace=True)

# Reset index to make date_time a column again before saving
df.reset_index(inplace=True)

# Save the DataFrame with the new features to a new CSV file
df.to_csv('historical_streamflow_data_with_features.csv', index=False)

display(df.head())

import torch
import torch.nn as nn

class FlashFloodClassifier(nn.Module):
    def __init__(self, input_dim):
        super(FlashFloodClassifier, self).__init__()
        self.layer_1 = nn.Linear(input_dim, 64)
        self.relu = nn.ReLU()
        self.layer_2 = nn.Linear(64, 32)
        self.layer_3 = nn.Linear(32, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.relu(self.layer_1(x))
        x = self.relu(self.layer_2(x))
        x = self.sigmoid(self.layer_3(x))
        return x

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
import torch.optim as optim

# 1. Load the historical streamflow data with the new features
df = pd.read_csv('historical_streamflow_data_with_features.csv')

# 2. Define features (X) and target variable (y)
features = ['streamflow_cfs', 'streamflow_p10', 'streamflow_p50', 'streamflow_p90', 'streamflow_diff', 'streamflow_pct_change']
X = df[features]

# Define the target variable: A simple approach could be to define a flash flood event
# as when the streamflow exceeds the 95th percentile of the entire dataset.
# This is a simplified definition and can be adjusted based on domain knowledge.
flood_threshold = df['streamflow_cfs'].quantile(0.95)
y = (df['streamflow_cfs'] > flood_threshold).astype(int)

# 3. Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)
X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)

# 6. Instantiate the FlashFloodClassifier model
input_dim = X_train_tensor.shape[1]
model = FlashFloodClassifier(input_dim)

# 7. Define the loss function and the optimizer
criterion = nn.BCELoss() # Binary Cross-Entropy Loss for binary classification
optimizer = optim.Adam(model.parameters(), lr=0.001) # Adam optimizer

# 8. Implement the training loop
num_epochs = 100
for epoch in range(num_epochs):
    # Forward pass
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)

    # Backward and optimize
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Print loss periodically
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# 9. Save the trained model and scaler
torch.save(model.state_dict(), 'flash_flood_model.pth')
import joblib
joblib.dump(scaler, 'scaler.pkl')

print("Training finished and model saved.")

import torch
import joblib
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import numpy as np

# 1. Load the trained model and the scaler
input_dim = X_test_tensor.shape[1] # Use the input dimension from the test tensor
model = FlashFloodClassifier(input_dim) # Instantiate the model with the correct input dimension
model.load_state_dict(torch.load('flash_flood_model.pth'))
scaler = joblib.load('scaler.pkl')

# 2. Put the loaded model in evaluation mode
model.eval()

# 3. Use the test set tensors to get predictions from the model
with torch.no_grad(): # Disable gradient calculation for inference
    y_pred_tensor = model(X_test_tensor)

# 4. Convert the model's output probabilities to binary predictions (0 or 1) using a threshold (e.g., 0.5)
y_pred_binary = (y_pred_tensor > 0.5).int()

# Convert tensors to numpy arrays for scikit-learn metrics
y_test_np = y_test_tensor.numpy()
y_pred_np = y_pred_binary.numpy()

# 5. Calculate and print the accuracy, precision, recall, and F1-score
accuracy = accuracy_score(y_test_np, y_pred_np)
precision = precision_score(y_test_np, y_pred_np)
recall = recall_score(y_test_np, y_pred_np)
f1 = f1_score(y_test_np, y_pred_np)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

# 6. Generate and display a confusion matrix
conf_matrix = confusion_matrix(y_test_np, y_pred_np)
print("\nConfusion Matrix:")
print(conf_matrix)

import torch
import joblib
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

def predict_flash_flood(model, scaler, site_number, prediction_date=None, lookback_days=7):
    """
    Predict flash flood probabilities for a specific location and date.

    Parameters:
    - model: Trained PyTorch model.
    - scaler: Fitted StandardScaler object.
    - site_number: USGS site number for the streamgage.
    - prediction_date: The date for which to predict the flash flood probability in 'YYYY-MM-DD' format. Defaults to today's date.
    - lookback_days: Number of days to look back from the prediction date to fetch data for feature calculation.

    Returns:
    - Predicted probability (a single value between 0 and 1) for the specified date, or None if not enough data.
    """
    # Determine the end date (prediction_date) and start date (lookback_days before prediction_date)
    if prediction_date is None:
        end_date = datetime.now()
    else:
        end_date = datetime.strptime(prediction_date, '%Y-%m-%d')

    start_date = (end_date - timedelta(days=lookback_days))

    end_date_str = end_date.strftime('%Y-%m-%d')
    start_date_str = start_date.strftime('%Y-%m-%d')

    # 1. Fetch historical streamflow data for the lookback period up to the prediction date
    historical_data = fetch_historical_streamflow_data(site_number, start_date_str, end_date_str)

    # Ensure the 'date_time' column is converted to datetime objects
    historical_data['date_time'] = pd.to_datetime(historical_data['date_time'])

    # Convert 'streamflow_cfs' to numeric, coercing errors to NaN
    historical_data['streamflow_cfs'] = pd.to_numeric(historical_data['streamflow_cfs'], errors='coerce')

    # Set 'date_time' as the index
    historical_data.set_index('date_time', inplace=True)

    # Sort the dataframe by index (date_time) to ensure correct rolling calculations
    historical_data.sort_index(inplace=True)

    # 2. Apply the same feature engineering steps as during training
    window_size = '7D' # Assuming daily data and a 7-day rolling window, match training

    historical_data['streamflow_p10'] = historical_data['streamflow_cfs'].rolling(window=window_size).quantile(0.10)
    historical_data['streamflow_p50'] = historical_data['streamflow_cfs'].rolling(window=window_size).quantile(0.50)
    historical_data['streamflow_p90'] = historical_data['streamflow_cfs'].rolling(window=window_size).quantile(0.90)

    historical_data['streamflow_diff'] = historical_data['streamflow_cfs'].diff()
    historical_data['streamflow_pct_change'] = historical_data['streamflow_cfs'].pct_change()

    # Handle missing values created by rolling window and diff/pct_change and potential NaNs from numeric conversion
    historical_data.dropna(inplace=True)

    # If after dropping NaNs, the dataframe is empty, we cannot make a prediction
    if historical_data.empty:
        print(f"Not enough data to generate features for prediction on {end_date_str}.")
        return None

    # 3. Select the feature columns for the data point on the prediction date
    features = ['streamflow_cfs', 'streamflow_p10', 'streamflow_p50', 'streamflow_p90', 'streamflow_diff', 'streamflow_pct_change']

    # Find the data point closest to the prediction date
    # We use asof to find the last observation on or before the prediction date
    latest_data_point = historical_data[features].asof(end_date)

    # Check if a data point was found for the prediction date
    if latest_data_point is None:
         print(f"No data available on or before {end_date_str} to generate features for prediction.")
         return None


    # 4. Use the loaded scaler to transform the feature data
    # latest_data_point is a Series, convert it to a list of lists for scaler.transform
    X_predict_scaled = scaler.transform([latest_data_point.values])

    # 5. Convert to PyTorch tensor
    X_predict_tensor = torch.tensor(X_predict_scaled, dtype=torch.float32)

    # 6. Set the model to evaluation mode
    model.eval()

    # 7. Perform inference with the model
    with torch.no_grad():
        prediction_proba = model(X_predict_tensor)

    # 8. Return the probability as a single value
    return prediction_proba.item()

# Example usage (assuming model and scaler are loaded from previous steps)
# Load the trained model and the scaler
input_dim = 6 # Based on the features used
model = FlashFloodClassifier(input_dim)
model.load_state_dict(torch.load('flash_flood_model.pth'))
scaler = joblib.load('scaler.pkl')

test_cases = [
    ("08166250", "2025-07-04"),  # Guadalupe River near Center Point, TX
    ("08166200", "2025-07-04"),  # Guadalupe River at Kerrville, TX
    ("08166140", "2003-08-30"),  # Example for Jacobs Creek / nearby KS site (placeholder)
]

for site_number, date_str in test_cases:
    specific_date_prediction_proba = predict_flash_flood(model, scaler, site_number, prediction_date=date_str, lookback_days=7)
    if specific_date_prediction_proba is not None:
        print(f"Site {site_number} on {date_str}: Predicted flash flood probability = {specific_date_prediction_proba:.4f}")

